{"id":"7d66be36-cc8e-4fa6-852e-6932cec69695","revision":0,"last_node_id":103,"last_link_id":192,"nodes":[{"id":8,"type":"DecodeTAESD","pos":[4180,3350],"size":[273.447265625,58],"flags":{"collapsed":true},"order":41,"mode":0,"inputs":[{"localized_name":"samples","name":"samples","type":"LATENT","link":8},{"localized_name":"taesd_model_name","name":"taesd_model_name","type":"COMBO","widget":{"name":"taesd_model_name"},"link":null}],"outputs":[{"localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","links":[49]}],"properties":{"Node name for S&R":"DecodeTAESD","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.5.2"}},"widgets_values":["taef1"],"color":"#322","bgcolor":"#533"},{"id":42,"type":"Text Concatenate","pos":[3750,3420],"size":[270,142],"flags":{"collapsed":true},"order":34,"mode":0,"inputs":[{"localized_name":"text_a","name":"text_a","shape":7,"type":"STRING","link":192},{"localized_name":"text_b","name":"text_b","shape":7,"type":"STRING","link":64},{"localized_name":"text_c","name":"text_c","shape":7,"type":"STRING","link":65},{"localized_name":"text_d","name":"text_d","shape":7,"type":"STRING","link":null},{"localized_name":"delimiter","name":"delimiter","type":"STRING","widget":{"name":"delimiter"},"link":null},{"localized_name":"clean_whitespace","name":"clean_whitespace","type":"COMBO","widget":{"name":"clean_whitespace"},"link":null}],"outputs":[{"localized_name":"STRING","name":"STRING","type":"STRING","links":[62,66]}],"properties":{"Node name for S&R":"Text Concatenate","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.5.2"}},"widgets_values":[", ","true"]},{"id":5,"type":"ConditioningZeroOut","pos":[4180,3270],"size":[204.134765625,26],"flags":{"collapsed":true},"order":38,"mode":0,"inputs":[{"localized_name":"conditioning","name":"conditioning","type":"CONDITIONING","link":172}],"outputs":[{"localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","links":[5,51]}],"properties":{"Node name for S&R":"ConditioningZeroOut","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.5.2"}},"widgets_values":[],"color":"#322","bgcolor":"#533"},{"id":39,"type":"DecodeTAESD","pos":[4420,3270],"size":[273.447265625,58],"flags":{"collapsed":true},"order":42,"mode":0,"inputs":[{"localized_name":"samples","name":"samples","type":"LATENT","link":54},{"localized_name":"taesd_model_name","name":"taesd_model_name","type":"COMBO","widget":{"name":"taesd_model_name"},"link":null}],"outputs":[{"localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","links":[55]}],"properties":{"Node name for S&R":"DecodeTAESD","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.5.2"}},"widgets_values":["taef1"],"color":"#322","bgcolor":"#533"},{"id":82,"type":"easy showAnything","pos":[2120,3120],"size":[300,550],"flags":{},"order":22,"mode":0,"inputs":[{"localized_name":"anything","name":"anything","shape":7,"type":"*","link":183}],"outputs":[{"localized_name":"output","name":"output","type":"*","links":null}],"title":"COMBINED ANALYSIS REPORT","properties":{"Node name for S&R":"easy showAnything","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.5.2"}},"widgets_values":["Block 05: ████ (6.99)\nBlock 06: ████ (7.00)\nBlock 07: ███ (5.92)\nBlock 08: ████ (7.42)\nBlock 09: ████ (7.85)\nBlock 10: ██████ (9.70)\nBlock 11: ██████ (9.91)\nBlock 12: ████ (7.43)\nBlock 13: ██████ (10.35)\nBlock 14: █████████ (15.89)\nBlock 15: ███████████ (19.09)\nBlock 16: ███████████ (18.69)\nBlock 17: ████████████████ (26.26)\nBlock 18: ████████████████ (26.05)\nBlock 24: ████████████████████ (31.99)\nBlock 25: ████████████ (20.18)"],"color":"#223","bgcolor":"#335"},{"id":75,"type":"easy showAnything","pos":[4660,2460],"size":[280,420],"flags":{},"order":28,"mode":0,"inputs":[{"localized_name":"anything","name":"anything","shape":7,"type":"*","link":156}],"outputs":[{"localized_name":"output","name":"output","type":"*","links":null}],"title":"COMBINED ANALYSIS REPORT","properties":{"Node name for S&R":"easy showAnything","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.5.2"}},"widgets_values":["### Z-FUSE GLOBAL IMPACT REPORT\nBlock 00: █████ (177.77)\nBlock 01: ██████ (206.85)\nBlock 02: ██████ (211.07)\nBlock 03: █████ (189.08)\nBlock 04: ██████ (211.00)\nBlock 05: ██████ (204.59)\nBlock 06: █████ (180.32)\nBlock 07: ████ (149.07)\nBlock 08: ████ (148.38)\nBlock 09: █ (61.13)\nBlock 10: ███ (117.27)\nBlock 11: ████ (141.78)\nBlock 12: ███ (126.80)\nBlock 13: ███ (120.81)\nBlock 14: ████ (142.13)\nBlock 15: █████ (179.88)\nBlock 16: █████ (169.66)\nBlock 17: █████████████████ (579.85)\nBlock 18: ███████████████████ (617.45)\nBlock 19: █████████████████ (563.78)\nBlock 20: ██████████████████ (596.87)\nBlock 21: ████████████████████ (647.34)\nBlock 22: ███████████████████ (634.87)\nBlock 23: █████████████████ (581.47)\nBlock 24: ██████████████████ (584.25)\nBlock 25: ████████████████ (529.24)\nBlock 26: ██████████████ (462.54)\nBlock 27: █████████████ (445.48)\nBlock 28: ██████████████ (480.93)\nBlock 29: ████████████ (391.47)\nContext:  (24.04)\nNoise:  (17.57)"],"color":"#223","bgcolor":"#335"},{"id":85,"type":"easy showAnything","pos":[1520,3120],"size":[300,550],"flags":{},"order":20,"mode":0,"inputs":[{"localized_name":"anything","name":"anything","shape":7,"type":"*","link":180}],"outputs":[{"localized_name":"output","name":"output","type":"*","links":null}],"title":"COMBINED ANALYSIS REPORT","properties":{"Node name for S&R":"easy showAnything","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.5.2"}},"widgets_values":["Block 00: ████████ (106.69)\nBlock 01: ██████████ (136.55)\nBlock 02: █████████ (128.35)\nBlock 03: ███████ (99.90)\nBlock 04: █████████ (118.72)\nBlock 05: █████ (76.41)\nBlock 06: ██████ (82.48)\nBlock 07: ███████ (99.11)\nBlock 08: ███████ (92.24)\nBlock 17: ███████████████ (198.76)\nBlock 18: ████████████████ (211.04)\nBlock 19: █████████████████ (234.11)\nBlock 20: ██████████████████ (244.25)\nBlock 21: ████████████████████ (260.69)\nBlock 22: ███████████████████ (259.71)\nBlock 23: ██████████████████ (241.07)\nBlock 24: ████████████████ (214.27)\nBlock 25: █████████████ (174.08)\nBlock 26: ████████████ (164.21)\nBlock 27: ███████████ (155.37)\nBlock 28: ████████████ (166.51)\nBlock 29: ██████████ (140.28)"],"color":"#223","bgcolor":"#335"},{"id":83,"type":"easy showAnything","pos":[2720,3120],"size":[300,550],"flags":{},"order":24,"mode":0,"inputs":[{"localized_name":"anything","name":"anything","shape":7,"type":"*","link":161}],"outputs":[{"localized_name":"output","name":"output","type":"*","links":null}],"title":"COMBINED ANALYSIS REPORT","properties":{"Node name for S&R":"easy showAnything","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.5.2"}},"widgets_values":["Block 00: █████ (8.86)\nBlock 01: █████ (8.74)\nBlock 02: ██████ (10.30)\nBlock 03: ███████ (11.12)\nBlock 04: ███████ (11.50)\nBlock 05: █████████ (14.23)\nBlock 06: ██████ (10.45)\nBlock 07: ██████ (9.48)\nBlock 08: ██████ (10.28)\nBlock 09: ███████ (11.34)\nBlock 10: ███████ (12.33)\nBlock 11: █████████ (14.98)\nBlock 12: ████████ (13.31)\nBlock 13: █████████ (14.14)\nBlock 14: ██████████ (16.01)\nBlock 15: ███████████ (18.54)\nBlock 16: █████████████ (20.61)\nBlock 17: ████████████ (19.81)\nBlock 18: ██████████████ (22.01)\nBlock 19: ███████████████ (23.69)\nBlock 20: █████████████████ (26.89)\nBlock 21: ████████████████████ (31.25)\nBlock 22: ██████████████████ (28.65)\nBlock 23: ███████████████ (24.63)\nBlock 24: ██████████████ (22.73)\nBlock 25: ███████████ (17.51)\nBlock 26: ██████████ (16.70)\nBlock 27: ██████████ (16.77)\nBlock 28: ███████████ (18.42)\nBlock 29: █████████████████ (27.58)\nContext: ███ (6.00)\nNoise: ██ (4.38)"],"color":"#223","bgcolor":"#335"},{"id":7,"type":"CLIPLoader","pos":[3430,2970],"size":[300,106],"flags":{},"order":0,"mode":0,"inputs":[{"localized_name":"clip_name","name":"clip_name","type":"COMBO","widget":{"name":"clip_name"},"link":null},{"localized_name":"type","name":"type","type":"COMBO","widget":{"name":"type"},"link":null},{"localized_name":"device","name":"device","shape":7,"type":"COMBO","widget":{"name":"device"},"link":null}],"outputs":[{"localized_name":"CLIP","name":"CLIP","type":"CLIP","links":[170]}],"properties":{"Node name for S&R":"CLIPLoader","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.5.2"}},"widgets_values":["qwen_3_4b.safetensors","lumina2","default"],"color":"#432","bgcolor":"#653"},{"id":90,"type":"CLIPSetLastLayer","pos":[3430,3120],"size":[210,58],"flags":{"collapsed":true},"order":16,"mode":0,"inputs":[{"localized_name":"clip","name":"clip","type":"CLIP","link":170},{"localized_name":"stop_at_clip_layer","name":"stop_at_clip_layer","type":"INT","widget":{"name":"stop_at_clip_layer"},"link":null}],"outputs":[{"localized_name":"CLIP","name":"CLIP","type":"CLIP","links":[169]}],"properties":{"Node name for S&R":"CLIPSetLastLayer","ue_properties":{"widget_ue_connectable":{"stop_at_clip_layer":true},"version":"7.1","input_ue_unconnectable":{}},"cnr_id":"comfy-core","ver":"0.3.76"},"widgets_values":[-1],"color":"#432","bgcolor":"#653"},{"id":4,"type":"CLIPTextEncode","pos":[3920,3420],"size":[400,88],"flags":{"collapsed":true},"order":35,"mode":0,"inputs":[{"localized_name":"clip","name":"clip","type":"CLIP","link":169},{"localized_name":"text","name":"text","type":"STRING","widget":{"name":"text"},"link":62}],"outputs":[{"localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","links":[174]}],"properties":{"Node name for S&R":"CLIPTextEncode","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.5.2"}},"widgets_values":["uwu_clarity"],"color":"#232","bgcolor":"#353"},{"id":9,"type":"PreviewImage","pos":[4960,2930],"size":[1550,1010],"flags":{},"order":44,"mode":0,"inputs":[{"localized_name":"images","name":"images","type":"IMAGE","link":48}],"outputs":[],"properties":{"Node name for S&R":"PreviewImage","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.5.2"}},"widgets_values":[]},{"id":46,"type":"Text Multiline","pos":[4190,3660],"size":[230,130],"flags":{},"order":1,"mode":0,"inputs":[{"localized_name":"text","name":"text","type":"STRING","widget":{"name":"text"},"link":null}],"outputs":[{"localized_name":"STRING","name":"STRING","type":"STRING","links":[189]}],"title":"Save Path","properties":{"Node name for S&R":"Text Multiline","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.5.2"}},"widgets_values":["D:\\Downloads\\Automatic1111\\ComfyUI_Portable\\output\\loras"],"color":"#2a363b","bgcolor":"#3f5159"},{"id":37,"type":"AILab_ImageCompare","pos":[4600,3360],"size":[300,242],"flags":{},"order":43,"mode":0,"inputs":[{"localized_name":"image1","name":"image1","shape":7,"type":"IMAGE","link":55},{"localized_name":"image2","name":"image2","shape":7,"type":"IMAGE","link":49},{"localized_name":"image3","name":"image3","shape":7,"type":"IMAGE","link":null},{"localized_name":"text1","name":"text1","type":"STRING","widget":{"name":"text1"},"link":null},{"localized_name":"text2","name":"text2","type":"STRING","widget":{"name":"text2"},"link":null},{"localized_name":"text3","name":"text3","type":"STRING","widget":{"name":"text3"},"link":null},{"localized_name":"size_base","name":"size_base","type":"COMBO","widget":{"name":"size_base"},"link":null},{"localized_name":"text_color","name":"text_color","type":"COLORCODE","widget":{"name":"text_color"},"link":null},{"localized_name":"bg_color","name":"bg_color","type":"COLORCODE","widget":{"name":"bg_color"},"link":null}],"outputs":[{"localized_name":"IMAGE","name":"IMAGE","type":"IMAGE","links":[48]}],"properties":{"Node name for S&R":"AILab_ImageCompare","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.5.2"}},"widgets_values":["Base Image","Merged Image","Image 3","largest","#000000","#FFFFFF"],"color":"#2e3e57","bgcolor":"#4b5b73"},{"id":88,"type":"EasyCache","pos":[3750,3350],"size":[210,130],"flags":{"collapsed":true},"order":25,"mode":0,"inputs":[{"localized_name":"model","name":"model","type":"MODEL","link":177},{"localized_name":"reuse_threshold","name":"reuse_threshold","type":"FLOAT","widget":{"name":"reuse_threshold"},"link":null},{"localized_name":"start_percent","name":"start_percent","type":"FLOAT","widget":{"name":"start_percent"},"link":null},{"localized_name":"end_percent","name":"end_percent","type":"FLOAT","widget":{"name":"end_percent"},"link":null},{"localized_name":"verbose","name":"verbose","type":"BOOLEAN","widget":{"name":"verbose"},"link":null}],"outputs":[{"localized_name":"MODEL","name":"MODEL","type":"MODEL","links":[168]}],"properties":{"Node name for S&R":"EasyCache","ue_properties":{"widget_ue_connectable":{"reuse_threshold":true,"start_percent":true,"end_percent":true,"verbose":true},"version":"7.1","input_ue_unconnectable":{}},"cnr_id":"comfy-core","ver":"0.3.76"},"widgets_values":[0.05,0.1,0.95,false]},{"id":94,"type":"ModelSamplingAuraFlow","pos":[3920,3350],"size":[210,60],"flags":{"collapsed":true},"order":31,"mode":0,"inputs":[{"localized_name":"model","name":"model","type":"MODEL","link":168},{"localized_name":"shift","name":"shift","type":"FLOAT","widget":{"name":"shift"},"link":null}],"outputs":[{"localized_name":"MODEL","name":"MODEL","type":"MODEL","slot_index":0,"links":[178]}],"properties":{"Node name for S&R":"ModelSamplingAuraFlow","ue_properties":{"widget_ue_connectable":{},"version":"7.1","input_ue_unconnectable":{}},"cnr_id":"comfy-core","ver":"0.3.64","enableTabs":false,"tabWidth":65,"tabXOffset":10,"hasSecondTab":false,"secondTabText":"Send Back","secondTabOffset":80,"secondTabWidth":65},"widgets_values":[3]},{"id":89,"type":"SeedVarianceEnhancer","pos":[3930,3470],"size":[302.73828125,250],"flags":{"collapsed":true},"order":37,"mode":0,"inputs":[{"localized_name":"conditioning","name":"conditioning","type":"CONDITIONING","link":174},{"localized_name":"randomize_percent","name":"randomize_percent","type":"FLOAT","widget":{"name":"randomize_percent"},"link":null},{"localized_name":"strength","name":"strength","type":"FLOAT","widget":{"name":"strength"},"link":null},{"localized_name":"noise_insert","name":"noise_insert","type":"COMBO","widget":{"name":"noise_insert"},"link":null},{"localized_name":"steps_switchover_percent","name":"steps_switchover_percent","type":"FLOAT","widget":{"name":"steps_switchover_percent"},"link":null},{"localized_name":"seed","name":"seed","type":"INT","widget":{"name":"seed"},"link":null},{"localized_name":"mask_starts_at","name":"mask_starts_at","type":"COMBO","widget":{"name":"mask_starts_at"},"link":null},{"localized_name":"mask_percent","name":"mask_percent","type":"FLOAT","widget":{"name":"mask_percent"},"link":null},{"localized_name":"log_to_console","name":"log_to_console","type":"BOOLEAN","widget":{"name":"log_to_console"},"link":null}],"outputs":[{"localized_name":"CONDITIONING","name":"CONDITIONING","type":"CONDITIONING","links":[171,172,173]}],"properties":{"Node name for S&R":"SeedVarianceEnhancer","ue_properties":{"widget_ue_connectable":{"randomize_percent":true,"strength":true,"noise_insert":true,"steps_switchover_percent":true,"seed":true,"mask_starts_at":true,"mask_percent":true,"log_to_console":true},"version":"7.1","input_ue_unconnectable":{}},"cnr_id":"seedvarianceenhancer","ver":"2.0.1"},"widgets_values":[50,20,"noise on beginning steps",20,0,"fixed","beginning",0,false],"color":"#232","bgcolor":"#353"},{"id":95,"type":"PrimitiveInt","pos":[4190,2970],"size":[270,82],"flags":{},"order":2,"mode":0,"inputs":[{"localized_name":"value","name":"value","type":"INT","widget":{"name":"value"},"link":null}],"outputs":[{"localized_name":"INT","name":"INT","type":"INT","links":[175,176]}],"properties":{"Node name for S&R":"PrimitiveInt","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.5.2"}},"widgets_values":[1,"fixed"],"color":"#2a363b","bgcolor":"#3f5159"},{"id":3,"type":"KSampler","pos":[4480,2970],"size":[210,262],"flags":{},"order":39,"mode":0,"inputs":[{"localized_name":"model","name":"model","type":"MODEL","link":178},{"localized_name":"positive","name":"positive","type":"CONDITIONING","link":171},{"localized_name":"negative","name":"negative","type":"CONDITIONING","link":5},{"localized_name":"latent_image","name":"latent_image","type":"LATENT","link":6},{"localized_name":"seed","name":"seed","type":"INT","widget":{"name":"seed"},"link":176},{"localized_name":"steps","name":"steps","type":"INT","widget":{"name":"steps"},"link":null},{"localized_name":"cfg","name":"cfg","type":"FLOAT","widget":{"name":"cfg"},"link":null},{"localized_name":"sampler_name","name":"sampler_name","type":"COMBO","widget":{"name":"sampler_name"},"link":null},{"localized_name":"scheduler","name":"scheduler","type":"COMBO","widget":{"name":"scheduler"},"link":null},{"localized_name":"denoise","name":"denoise","type":"FLOAT","widget":{"name":"denoise"},"link":null}],"outputs":[{"localized_name":"LATENT","name":"LATENT","type":"LATENT","links":[8]}],"properties":{"Node name for S&R":"KSampler","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.5.2"}},"widgets_values":[1,"fixed",8,1,"euler","beta",1],"color":"#2a363b","bgcolor":"#3f5159"},{"id":38,"type":"KSampler","pos":[4710,2970],"size":[210,262],"flags":{},"order":40,"mode":0,"inputs":[{"localized_name":"model","name":"model","type":"MODEL","link":53},{"localized_name":"positive","name":"positive","type":"CONDITIONING","link":173},{"localized_name":"negative","name":"negative","type":"CONDITIONING","link":51},{"localized_name":"latent_image","name":"latent_image","type":"LATENT","link":52},{"localized_name":"seed","name":"seed","type":"INT","widget":{"name":"seed"},"link":175},{"localized_name":"steps","name":"steps","type":"INT","widget":{"name":"steps"},"link":null},{"localized_name":"cfg","name":"cfg","type":"FLOAT","widget":{"name":"cfg"},"link":null},{"localized_name":"sampler_name","name":"sampler_name","type":"COMBO","widget":{"name":"sampler_name"},"link":null},{"localized_name":"scheduler","name":"scheduler","type":"COMBO","widget":{"name":"scheduler"},"link":null},{"localized_name":"denoise","name":"denoise","type":"FLOAT","widget":{"name":"denoise"},"link":null}],"outputs":[{"localized_name":"LATENT","name":"LATENT","type":"LATENT","links":[54]}],"properties":{"Node name for S&R":"KSampler","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.5.2"}},"widgets_values":[1,"fixed",8,1,"euler","beta",1],"color":"#2a363b","bgcolor":"#3f5159"},{"id":6,"type":"EmptySD3LatentImage","pos":[4190,3500],"size":[230,110],"flags":{"collapsed":false},"order":3,"mode":0,"inputs":[{"localized_name":"width","name":"width","type":"INT","widget":{"name":"width"},"link":null},{"localized_name":"height","name":"height","type":"INT","widget":{"name":"height"},"link":null},{"localized_name":"batch_size","name":"batch_size","type":"INT","widget":{"name":"batch_size"},"link":null}],"outputs":[{"localized_name":"LATENT","name":"LATENT","type":"LATENT","links":[6,52]}],"properties":{"Node name for S&R":"EmptySD3LatentImage","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.5.2"}},"widgets_values":[1280,1280,1],"color":"#323","bgcolor":"#535"},{"id":43,"type":"Text Multiline","pos":[3740,3600],"size":[300,130],"flags":{},"order":4,"mode":0,"inputs":[{"localized_name":"text","name":"text","type":"STRING","widget":{"name":"text"},"link":null}],"outputs":[{"localized_name":"STRING","name":"STRING","type":"STRING","links":[64]}],"title":"TAGS","properties":{"Node name for S&R":"Text Multiline","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.5.2"}},"widgets_values":["uwu_clarity, masterpiece, best quality, very aesthetic,"],"color":"#232","bgcolor":"#353"},{"id":79,"type":"ZFuseOrchestrator","pos":[3120,2970],"size":[300,170],"flags":{},"order":23,"mode":0,"inputs":[{"localized_name":"model","name":"model","type":"MODEL","link":151},{"localized_name":"lora_stack","name":"lora_stack","type":"LORA_STACK","link":162},{"localized_name":"merge_mode","name":"merge_mode","type":"COMBO","widget":{"name":"merge_mode"},"link":null},{"localized_name":"ties_threshold","name":"ties_threshold","type":"FLOAT","widget":{"name":"ties_threshold"},"link":null}],"outputs":[{"localized_name":"MODEL","name":"MODEL","type":"MODEL","links":[177,188]},{"localized_name":"TRIGGER_WORDS","name":"TRIGGER_WORDS","type":"STRING","links":[155]},{"localized_name":"ANALYSIS_REPORT","name":"ANALYSIS_REPORT","type":"STRING","links":[156]},{"localized_name":"STATUS_LOG","name":"STATUS_LOG","type":"STRING","links":[157]},{"localized_name":"DUMP_INFO","name":"DUMP_INFO","type":"STRING","links":[158]}],"properties":{"Node name for S&R":"ZFuseOrchestrator","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.5.2"}},"widgets_values":["TIES",0.1],"color":"#323","bgcolor":"#535"},{"id":87,"type":"Note","pos":[3120,3180],"size":[550,170],"flags":{},"order":5,"mode":0,"inputs":[],"outputs":[],"title":"TIES","properties":{"ue_properties":{"widget_ue_connectable":{},"version":"7.5.2","input_ue_unconnectable":{}}},"widgets_values":["1. How does the TIES Threshold effect strength?\nThe ties_threshold does not directly lower or raise the strength of your LoRA weights. It acts as a \"Noise Filter.\"\n\nIncreasing the Threshold (e.g., 0.2 → 0.5):\nMore Pruning: You are telling the system, \"Only pay attention to the most significant changes (the top 50%).\"\nResult: The output will look \"cleaner\" and sharper, but if you go too high, you might lose subtle details, skin textures, or fine lighting because the node treats them as \"noise\" and deletes them.\nStrength: The visual impact often appears stronger because weak, conflicting signals are removed, leaving only the strong, agreed-upon changes.\n\nDecreasing the Threshold (e.g., 0.2 → 0.05):\nLess Pruning: You are telling the system, \"Keep almost everything, even tiny changes.\"\nResult: More detail is preserved, but it also keeps mathematical \"junk\" that might cause the \"blurry soup\" or over-saturation.\nStrength: The image might look softer or more washed out because many weak signals are being averaged together."],"color":"#432","bgcolor":"#653"},{"id":96,"type":"ZFuseVisualLayerTuner","pos":[1230,3060],"size":[281.0728515625,822],"flags":{},"order":17,"mode":0,"inputs":[{"localized_name":"lora_stack","name":"lora_stack","shape":7,"type":"LORA_STACK","link":186},{"localized_name":"lora_name","name":"lora_name","type":"COMBO","widget":{"name":"lora_name"},"link":null},{"localized_name":"base_strength","name":"base_strength","type":"FLOAT","widget":{"name":"base_strength"},"link":null},{"localized_name":"block_0","name":"block_0","type":"FLOAT","widget":{"name":"block_0"},"link":null},{"localized_name":"block_1","name":"block_1","type":"FLOAT","widget":{"name":"block_1"},"link":null},{"localized_name":"block_2","name":"block_2","type":"FLOAT","widget":{"name":"block_2"},"link":null},{"localized_name":"block_3","name":"block_3","type":"FLOAT","widget":{"name":"block_3"},"link":null},{"localized_name":"block_4","name":"block_4","type":"FLOAT","widget":{"name":"block_4"},"link":null},{"localized_name":"block_5","name":"block_5","type":"FLOAT","widget":{"name":"block_5"},"link":null},{"localized_name":"block_6","name":"block_6","type":"FLOAT","widget":{"name":"block_6"},"link":null},{"localized_name":"block_7","name":"block_7","type":"FLOAT","widget":{"name":"block_7"},"link":null},{"localized_name":"block_8","name":"block_8","type":"FLOAT","widget":{"name":"block_8"},"link":null},{"localized_name":"block_9","name":"block_9","type":"FLOAT","widget":{"name":"block_9"},"link":null},{"localized_name":"block_10","name":"block_10","type":"FLOAT","widget":{"name":"block_10"},"link":null},{"localized_name":"block_11","name":"block_11","type":"FLOAT","widget":{"name":"block_11"},"link":null},{"localized_name":"block_12","name":"block_12","type":"FLOAT","widget":{"name":"block_12"},"link":null},{"localized_name":"block_13","name":"block_13","type":"FLOAT","widget":{"name":"block_13"},"link":null},{"localized_name":"block_14","name":"block_14","type":"FLOAT","widget":{"name":"block_14"},"link":null},{"localized_name":"block_15","name":"block_15","type":"FLOAT","widget":{"name":"block_15"},"link":null},{"localized_name":"block_16","name":"block_16","type":"FLOAT","widget":{"name":"block_16"},"link":null},{"localized_name":"block_17","name":"block_17","type":"FLOAT","widget":{"name":"block_17"},"link":null},{"localized_name":"block_18","name":"block_18","type":"FLOAT","widget":{"name":"block_18"},"link":null},{"localized_name":"block_19","name":"block_19","type":"FLOAT","widget":{"name":"block_19"},"link":null},{"localized_name":"block_20","name":"block_20","type":"FLOAT","widget":{"name":"block_20"},"link":null},{"localized_name":"block_21","name":"block_21","type":"FLOAT","widget":{"name":"block_21"},"link":null},{"localized_name":"block_22","name":"block_22","type":"FLOAT","widget":{"name":"block_22"},"link":null},{"localized_name":"block_23","name":"block_23","type":"FLOAT","widget":{"name":"block_23"},"link":null},{"localized_name":"block_24","name":"block_24","type":"FLOAT","widget":{"name":"block_24"},"link":null},{"localized_name":"block_25","name":"block_25","type":"FLOAT","widget":{"name":"block_25"},"link":null},{"localized_name":"block_26","name":"block_26","type":"FLOAT","widget":{"name":"block_26"},"link":null},{"localized_name":"block_27","name":"block_27","type":"FLOAT","widget":{"name":"block_27"},"link":null},{"localized_name":"block_28","name":"block_28","type":"FLOAT","widget":{"name":"block_28"},"link":null},{"localized_name":"block_29","name":"block_29","type":"FLOAT","widget":{"name":"block_29"},"link":null}],"outputs":[{"localized_name":"LORA_STACK","name":"LORA_STACK","type":"LORA_STACK","links":[181]},{"localized_name":"ANALYSIS_REPORT","name":"ANALYSIS_REPORT","type":"STRING","links":[180]}],"properties":{"Node name for S&R":"ZFuseVisualLayerTuner","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.5.2"}},"widgets_values":["Mystic-XXX-ZIT-v3.safetensors",1,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1],"color":"#323","bgcolor":"#535"},{"id":97,"type":"ZFuseVisualLayerTuner","pos":[1830,3060],"size":[281.0728515625,822],"flags":{},"order":19,"mode":0,"inputs":[{"localized_name":"lora_stack","name":"lora_stack","shape":7,"type":"LORA_STACK","link":181},{"localized_name":"lora_name","name":"lora_name","type":"COMBO","widget":{"name":"lora_name"},"link":null},{"localized_name":"base_strength","name":"base_strength","type":"FLOAT","widget":{"name":"base_strength"},"link":null},{"localized_name":"block_0","name":"block_0","type":"FLOAT","widget":{"name":"block_0"},"link":null},{"localized_name":"block_1","name":"block_1","type":"FLOAT","widget":{"name":"block_1"},"link":null},{"localized_name":"block_2","name":"block_2","type":"FLOAT","widget":{"name":"block_2"},"link":null},{"localized_name":"block_3","name":"block_3","type":"FLOAT","widget":{"name":"block_3"},"link":null},{"localized_name":"block_4","name":"block_4","type":"FLOAT","widget":{"name":"block_4"},"link":null},{"localized_name":"block_5","name":"block_5","type":"FLOAT","widget":{"name":"block_5"},"link":null},{"localized_name":"block_6","name":"block_6","type":"FLOAT","widget":{"name":"block_6"},"link":null},{"localized_name":"block_7","name":"block_7","type":"FLOAT","widget":{"name":"block_7"},"link":null},{"localized_name":"block_8","name":"block_8","type":"FLOAT","widget":{"name":"block_8"},"link":null},{"localized_name":"block_9","name":"block_9","type":"FLOAT","widget":{"name":"block_9"},"link":null},{"localized_name":"block_10","name":"block_10","type":"FLOAT","widget":{"name":"block_10"},"link":null},{"localized_name":"block_11","name":"block_11","type":"FLOAT","widget":{"name":"block_11"},"link":null},{"localized_name":"block_12","name":"block_12","type":"FLOAT","widget":{"name":"block_12"},"link":null},{"localized_name":"block_13","name":"block_13","type":"FLOAT","widget":{"name":"block_13"},"link":null},{"localized_name":"block_14","name":"block_14","type":"FLOAT","widget":{"name":"block_14"},"link":null},{"localized_name":"block_15","name":"block_15","type":"FLOAT","widget":{"name":"block_15"},"link":null},{"localized_name":"block_16","name":"block_16","type":"FLOAT","widget":{"name":"block_16"},"link":null},{"localized_name":"block_17","name":"block_17","type":"FLOAT","widget":{"name":"block_17"},"link":null},{"localized_name":"block_18","name":"block_18","type":"FLOAT","widget":{"name":"block_18"},"link":null},{"localized_name":"block_19","name":"block_19","type":"FLOAT","widget":{"name":"block_19"},"link":null},{"localized_name":"block_20","name":"block_20","type":"FLOAT","widget":{"name":"block_20"},"link":null},{"localized_name":"block_21","name":"block_21","type":"FLOAT","widget":{"name":"block_21"},"link":null},{"localized_name":"block_22","name":"block_22","type":"FLOAT","widget":{"name":"block_22"},"link":null},{"localized_name":"block_23","name":"block_23","type":"FLOAT","widget":{"name":"block_23"},"link":null},{"localized_name":"block_24","name":"block_24","type":"FLOAT","widget":{"name":"block_24"},"link":null},{"localized_name":"block_25","name":"block_25","type":"FLOAT","widget":{"name":"block_25"},"link":null},{"localized_name":"block_26","name":"block_26","type":"FLOAT","widget":{"name":"block_26"},"link":null},{"localized_name":"block_27","name":"block_27","type":"FLOAT","widget":{"name":"block_27"},"link":null},{"localized_name":"block_28","name":"block_28","type":"FLOAT","widget":{"name":"block_28"},"link":null},{"localized_name":"block_29","name":"block_29","type":"FLOAT","widget":{"name":"block_29"},"link":null}],"outputs":[{"localized_name":"LORA_STACK","name":"LORA_STACK","type":"LORA_STACK","links":[182]},{"localized_name":"ANALYSIS_REPORT","name":"ANALYSIS_REPORT","type":"STRING","links":[183]}],"properties":{"Node name for S&R":"ZFuseVisualLayerTuner","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.5.2"}},"widgets_values":["Olympus.safetensors",1,0,0,0,0,0,1,1,1,1,1,1,1,1,0.5,0.5,0.5,0.5,0.25,0.25,0,0,0,0,0,1,1,0,0,0,0],"color":"#323","bgcolor":"#535"},{"id":2,"type":"UNETLoader","pos":[2730,2970],"size":[290,90],"flags":{},"order":6,"mode":0,"inputs":[{"localized_name":"unet_name","name":"unet_name","type":"COMBO","widget":{"name":"unet_name"},"link":null},{"localized_name":"weight_dtype","name":"weight_dtype","type":"COMBO","widget":{"name":"weight_dtype"},"link":null}],"outputs":[{"localized_name":"MODEL","name":"MODEL","type":"MODEL","links":[53,151]}],"properties":{"Node name for S&R":"UNETLoader","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.5.2"}},"widgets_values":["z-image-turbo-fp8-e4m3fn.safetensors","fp8_e4m3fn_fast"],"color":"#223","bgcolor":"#335"},{"id":71,"type":"ZFuseVisualLayerTuner","pos":[2430,3060],"size":[281.0728515625,822],"flags":{},"order":21,"mode":0,"inputs":[{"localized_name":"lora_stack","name":"lora_stack","shape":7,"type":"LORA_STACK","link":182},{"localized_name":"lora_name","name":"lora_name","type":"COMBO","widget":{"name":"lora_name"},"link":null},{"localized_name":"base_strength","name":"base_strength","type":"FLOAT","widget":{"name":"base_strength"},"link":null},{"localized_name":"block_0","name":"block_0","type":"FLOAT","widget":{"name":"block_0"},"link":null},{"localized_name":"block_1","name":"block_1","type":"FLOAT","widget":{"name":"block_1"},"link":null},{"localized_name":"block_2","name":"block_2","type":"FLOAT","widget":{"name":"block_2"},"link":null},{"localized_name":"block_3","name":"block_3","type":"FLOAT","widget":{"name":"block_3"},"link":null},{"localized_name":"block_4","name":"block_4","type":"FLOAT","widget":{"name":"block_4"},"link":null},{"localized_name":"block_5","name":"block_5","type":"FLOAT","widget":{"name":"block_5"},"link":null},{"localized_name":"block_6","name":"block_6","type":"FLOAT","widget":{"name":"block_6"},"link":null},{"localized_name":"block_7","name":"block_7","type":"FLOAT","widget":{"name":"block_7"},"link":null},{"localized_name":"block_8","name":"block_8","type":"FLOAT","widget":{"name":"block_8"},"link":null},{"localized_name":"block_9","name":"block_9","type":"FLOAT","widget":{"name":"block_9"},"link":null},{"localized_name":"block_10","name":"block_10","type":"FLOAT","widget":{"name":"block_10"},"link":null},{"localized_name":"block_11","name":"block_11","type":"FLOAT","widget":{"name":"block_11"},"link":null},{"localized_name":"block_12","name":"block_12","type":"FLOAT","widget":{"name":"block_12"},"link":null},{"localized_name":"block_13","name":"block_13","type":"FLOAT","widget":{"name":"block_13"},"link":null},{"localized_name":"block_14","name":"block_14","type":"FLOAT","widget":{"name":"block_14"},"link":null},{"localized_name":"block_15","name":"block_15","type":"FLOAT","widget":{"name":"block_15"},"link":null},{"localized_name":"block_16","name":"block_16","type":"FLOAT","widget":{"name":"block_16"},"link":null},{"localized_name":"block_17","name":"block_17","type":"FLOAT","widget":{"name":"block_17"},"link":null},{"localized_name":"block_18","name":"block_18","type":"FLOAT","widget":{"name":"block_18"},"link":null},{"localized_name":"block_19","name":"block_19","type":"FLOAT","widget":{"name":"block_19"},"link":null},{"localized_name":"block_20","name":"block_20","type":"FLOAT","widget":{"name":"block_20"},"link":null},{"localized_name":"block_21","name":"block_21","type":"FLOAT","widget":{"name":"block_21"},"link":null},{"localized_name":"block_22","name":"block_22","type":"FLOAT","widget":{"name":"block_22"},"link":null},{"localized_name":"block_23","name":"block_23","type":"FLOAT","widget":{"name":"block_23"},"link":null},{"localized_name":"block_24","name":"block_24","type":"FLOAT","widget":{"name":"block_24"},"link":null},{"localized_name":"block_25","name":"block_25","type":"FLOAT","widget":{"name":"block_25"},"link":null},{"localized_name":"block_26","name":"block_26","type":"FLOAT","widget":{"name":"block_26"},"link":null},{"localized_name":"block_27","name":"block_27","type":"FLOAT","widget":{"name":"block_27"},"link":null},{"localized_name":"block_28","name":"block_28","type":"FLOAT","widget":{"name":"block_28"},"link":null},{"localized_name":"block_29","name":"block_29","type":"FLOAT","widget":{"name":"block_29"},"link":null}],"outputs":[{"localized_name":"LORA_STACK","name":"LORA_STACK","type":"LORA_STACK","links":[162]},{"localized_name":"ANALYSIS_REPORT","name":"ANALYSIS_REPORT","type":"STRING","links":[161]}],"properties":{"Node name for S&R":"ZFuseVisualLayerTuner","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.5.2"}},"widgets_values":["UwU_Clarity_ZiT_v4.0_rank_16_fp16_00001_.safetensors",2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,1],"color":"#323","bgcolor":"#535"},{"id":99,"type":"easy showAnything","pos":[920,3120],"size":[300,550],"flags":{},"order":18,"mode":0,"inputs":[{"localized_name":"anything","name":"anything","shape":7,"type":"*","link":187}],"outputs":[{"localized_name":"output","name":"output","type":"*","links":null}],"title":"COMBINED ANALYSIS REPORT","properties":{"Node name for S&R":"easy showAnything","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.5.2"}},"widgets_values":["Block 10: █████████ (16.15)\nBlock 11: ███████████ (20.63)\nBlock 12: ███████████ (19.51)\nBlock 13: ███████████ (19.19)\nBlock 14: ████████████ (22.15)\nBlock 15: █████████████████ (30.88)\nBlock 16: █████████████████ (29.69)\nBlock 17: ███████████████████ (34.26)\nBlock 18: ████████████████████ (34.68)"],"color":"#223","bgcolor":"#335"},{"id":98,"type":"Note","pos":[980,2460],"size":[450,360],"flags":{},"order":7,"mode":0,"inputs":[],"outputs":[],"title":"Setting block weights","properties":{"ue_properties":{"widget_ue_connectable":{},"version":"7.5.2","input_ue_unconnectable":{}}},"widgets_values":["* Values can range from -10 up to +10\n* A value of 0 skips the block from being calculated.\n\nSkipping blocks saves a ton of RAM.\n\nI recommend getting a report for each lora. Any weak blocks just outright disable.\n\nFocus on medium and heavy blocks.\n\nThis balances speed and performance and prevents high RAM utilization.\n\nBASE STRENTH:\n\nMultiplies all blocks by the value.\n\n2*1=2\n\nIt makes the overall lora have more impact. I would try to do things individually before just upping the base strength.\n"],"color":"#432","bgcolor":"#653"},{"id":69,"type":"Note","pos":[1440,2460],"size":[400,360],"flags":{},"order":8,"mode":0,"inputs":[],"outputs":[],"title":"Visual Feedback","properties":{"ue_properties":{"widget_ue_connectable":{},"version":"7.5.2","input_ue_unconnectable":{}}},"widgets_values":["We can see how much data is contained in every block. Noramlly when loading many loras you push these values past a point of saturation. This is what makes loading many loras break your generation.\n\nWe now can visualize \"hot\" and \"cold\" layers. Allowing you to shift the lora in places you'd want and alternatively decrease area's where you want other loras to shine.\n\nIn addition to this we merge these weights algorithmically ensuring data that is conflicting or similar is reconstructed in a nondestuctive manner. This fuses data and makes them play nice when you want to really saturate blocks without completely destoying them. Blending them into something new."],"color":"#432","bgcolor":"#653"},{"id":80,"type":"Note","pos":[2270,2460],"size":[480,360],"flags":{},"order":9,"mode":0,"inputs":[],"outputs":[],"title":"VLT vs LBS nodes","properties":{"ue_properties":{"widget_ue_connectable":{},"version":"7.5.2","input_ue_unconnectable":{}}},"widgets_values":["These two node act similar but aren't exactly the same. One offers a mass movement of many blocks with a standard set weight, while the other allows you to shift block individually for more granular control.\n\nVisual Layer Tuner:\n-Exposes all block layers that you can shift individually\n-Large node that takes up a lot of room on the grid\n\nLoRA Bus Stacker:\n-Lets you set a range of blocks to affect\n-Has a manual override that lets you change the weight of specific blocks\n[12:1.5 - This would be Block_12 with a strength of 1.5]\n-Compact node that doesn't take up much space in the graph"],"color":"#432","bgcolor":"#653"},{"id":35,"type":"Note","pos":[2760,2460],"size":[310,360],"flags":{},"order":10,"mode":0,"inputs":[],"outputs":[],"title":"Z-FUSE","properties":{"ue_properties":{"widget_ue_connectable":{},"version":"7.5.2","input_ue_unconnectable":{}}},"widgets_values":["Z-FUSE intelligently merges loras by blocks with a different merge method from the standard LoadLoraModelOnly node.\n\nThe LoadLoraModelOnly uses sequential patching logic, while we instead merge by blocks. The LoadLoraModelOnly node and it's patching logic stacks loras and pushes the entire model which causes mathmatical saturation. This is why many loras loaded with the standard node cause images to distort. A patch on a Patch quickly causes the model to break since you're moving many paramters all at once with no regard for what blocks should actually be shifted or how they are shifted."],"color":"#432","bgcolor":"#653"},{"id":76,"type":"easy showAnything","pos":[4270,2460],"size":[390,420],"flags":{},"order":30,"mode":0,"inputs":[{"localized_name":"anything","name":"anything","shape":7,"type":"*","link":158}],"outputs":[{"localized_name":"output","name":"output","type":"*","links":null}],"title":"Info dump - DEBUGGING","properties":{"Node name for S&R":"easy showAnything","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.5.2"}},"widgets_values":["\n[UwU_Clarity_Zimg_000018750.safetensors] Sample Keys:\n  diffusion_model.layers.0.adaLN_modulation.0.lora_A.weight\n  diffusion_model.layers.0.adaLN_modulation.0.lora_B.weight\n  diffusion_model.layers.0.attention.to_k.lora_A.weight\n  diffusion_model.layers.0.attention.to_k.lora_B.weight\n  diffusion_model.layers.0.attention.to_out.0.lora_A.weight\n  diffusion_model.layers.0.attention.to_out.0.lora_B.weight\n  diffusion_model.layers.0.attention.to_q.lora_A.weight\n  diffusion_model.layers.0.attention.to_q.lora_B.weight\n  diffusion_model.layers.0.attention.to_v.lora_A.weight\n  diffusion_model.layers.0.attention.to_v.lora_B.weight\n  [MAP TEST] LoRA Key: diffusion_model.layers.10.adaLN_modulation.0.lora_A.weight\n  [MAP TEST] Target Key: diffusion_model.layers.10.adaLN_modulation.0.weight\n  [MAP TEST] LoRA Key: diffusion_model.layers.10.attention.to_k.lora_A.weight\n  [MAP TEST] Target Key: diffusion_model.layers.10.attention.to_k.weight\n  [MAP TEST] LoRA Key: diffusion_model.layers.10.attention.to_out.0.lora_A.weight\n  [MAP TEST] Target Key: diffusion_model.layers.10.attention.to_out.0.weight\n\n[Mystic-XXX-ZIT-v3.safetensors] Sample Keys:\n  diffusion_model.layers.0.adaLN_modulation.0.lora_A.weight\n  diffusion_model.layers.0.adaLN_modulation.0.lora_B.weight\n  diffusion_model.layers.0.attention.to_k.lora_A.weight\n  diffusion_model.layers.0.attention.to_k.lora_B.weight\n  diffusion_model.layers.0.attention.to_out.0.lora_A.weight\n  diffusion_model.layers.0.attention.to_out.0.lora_B.weight\n  diffusion_model.layers.0.attention.to_q.lora_A.weight\n  diffusion_model.layers.0.attention.to_q.lora_B.weight\n  diffusion_model.layers.0.attention.to_v.lora_A.weight\n  diffusion_model.layers.0.attention.to_v.lora_B.weight\n  [MAP TEST] LoRA Key: diffusion_model.layers.0.adaLN_modulation.0.lora_A.weight\n  [MAP TEST] Target Key: diffusion_model.layers.0.adaLN_modulation.0.weight\n  [MAP TEST] LoRA Key: diffusion_model.layers.0.attention.to_k.lora_A.weight\n  [MAP TEST] Target Key: diffusion_model.layers.0.attention.to_k.weight\n  [MAP TEST] LoRA Key: diffusion_model.layers.0.attention.to_out.0.lora_A.weight\n  [MAP TEST] Target Key: diffusion_model.layers.0.attention.to_out.0.weight\n\n[Olympus.safetensors] Sample Keys:\n  diffusion_model.layers.0.adaLN_modulation.0.lora_A.weight\n  diffusion_model.layers.0.adaLN_modulation.0.lora_B.weight\n  diffusion_model.layers.0.attention.to_k.lora_A.weight\n  diffusion_model.layers.0.attention.to_k.lora_B.weight\n  diffusion_model.layers.0.attention.to_out.0.lora_A.weight\n  diffusion_model.layers.0.attention.to_out.0.lora_B.weight\n  diffusion_model.layers.0.attention.to_q.lora_A.weight\n  diffusion_model.layers.0.attention.to_q.lora_B.weight\n  diffusion_model.layers.0.attention.to_v.lora_A.weight\n  diffusion_model.layers.0.attention.to_v.lora_B.weight\n  [MAP TEST] LoRA Key: diffusion_model.layers.10.adaLN_modulation.0.lora_A.weight\n  [MAP TEST] Target Key: diffusion_model.layers.10.adaLN_modulation.0.weight\n  [MAP TEST] LoRA Key: diffusion_model.layers.10.attention.to_k.lora_A.weight\n  [MAP TEST] Target Key: diffusion_model.layers.10.attention.to_k.weight\n  [MAP TEST] LoRA Key: diffusion_model.layers.10.attention.to_out.0.lora_A.weight\n  [MAP TEST] Target Key: diffusion_model.layers.10.attention.to_out.0.weight\n\n[UwU_Clarity_ZiT_v4.0_rank_16_fp16_00001_.safetensors] Sample Keys:\n  diffusion_model.cap_embedder.0.diff\n  diffusion_model.cap_embedder.1.diff_b\n  diffusion_model.cap_embedder.1.lora_down.weight\n  diffusion_model.cap_embedder.1.lora_up.weight\n  diffusion_model.context_refiner.0.attention.k_norm.diff\n  diffusion_model.context_refiner.0.attention.out.lora_down.weight\n  diffusion_model.context_refiner.0.attention.out.lora_up.weight\n  diffusion_model.context_refiner.0.attention.q_norm.diff\n  diffusion_model.context_refiner.0.attention.qkv.lora_down.weight\n  diffusion_model.context_refiner.0.attention.qkv.lora_up.weight\n  [MAP TEST] LoRA Key: diffusion_model.cap_embedder.1.lora_up.weight\n  [MAP TEST] Target Key: diffusion_model.cap_embedder.1.weight\n  [MAP TEST] LoRA Key: diffusion_model.context_refiner.0.attention.out.lora_up.weight\n  [MAP TEST] Target Key: diffusion_model.context_refiner.0.attention.out.weight\n  [MAP TEST] LoRA Key: diffusion_model.context_refiner.0.attention.qkv.lora_up.weight\n  [MAP TEST] Target Key: diffusion_model.context_refiner.0.attention.qkv.weight"],"color":"#223","bgcolor":"#335"},{"id":74,"type":"easy showAnything","pos":[3970,2460],"size":[300,420],"flags":{},"order":29,"mode":0,"inputs":[{"localized_name":"anything","name":"anything","shape":7,"type":"*","link":157}],"outputs":[{"localized_name":"output","name":"output","type":"*","links":null}],"title":"STATUS LOG","properties":{"Node name for S&R":"easy showAnything","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.5.2"}},"widgets_values":["--- Z-FUSE ORCHESTRATION INITIATED ---\nModel Architecture detected. Prefix set to: 'diffusion_model.'\nProcessing: UwU_Clarity_Zimg_000018750.safetensors (visual_tuner)\n  > Mapped 72 keys.\nProcessing: Mystic-XXX-ZIT-v3.safetensors (visual_tuner)\n  > Mapped 176 keys.\nProcessing: Olympus.safetensors (visual_tuner)\n  > Mapped 128 keys.\nProcessing: UwU_Clarity_ZiT_v4.0_rank_16_fp16_00001_.safetensors (visual_tuner)\n  > Mapped 208 keys.\nFusing 328 unique tensors using TIES...\nDEBUG: 208 of 328 keys matched the model.\nSuccessfully applied 328 patched layers.\n--- DONE ---"],"color":"#223","bgcolor":"#335"},{"id":101,"type":"ZFuseVisualLayerTuner","pos":[620,3060],"size":[281.0728515625,822],"flags":{},"order":11,"mode":0,"inputs":[{"localized_name":"lora_stack","name":"lora_stack","shape":7,"type":"LORA_STACK","link":null},{"localized_name":"lora_name","name":"lora_name","type":"COMBO","widget":{"name":"lora_name"},"link":null},{"localized_name":"base_strength","name":"base_strength","type":"FLOAT","widget":{"name":"base_strength"},"link":null},{"localized_name":"block_0","name":"block_0","type":"FLOAT","widget":{"name":"block_0"},"link":null},{"localized_name":"block_1","name":"block_1","type":"FLOAT","widget":{"name":"block_1"},"link":null},{"localized_name":"block_2","name":"block_2","type":"FLOAT","widget":{"name":"block_2"},"link":null},{"localized_name":"block_3","name":"block_3","type":"FLOAT","widget":{"name":"block_3"},"link":null},{"localized_name":"block_4","name":"block_4","type":"FLOAT","widget":{"name":"block_4"},"link":null},{"localized_name":"block_5","name":"block_5","type":"FLOAT","widget":{"name":"block_5"},"link":null},{"localized_name":"block_6","name":"block_6","type":"FLOAT","widget":{"name":"block_6"},"link":null},{"localized_name":"block_7","name":"block_7","type":"FLOAT","widget":{"name":"block_7"},"link":null},{"localized_name":"block_8","name":"block_8","type":"FLOAT","widget":{"name":"block_8"},"link":null},{"localized_name":"block_9","name":"block_9","type":"FLOAT","widget":{"name":"block_9"},"link":null},{"localized_name":"block_10","name":"block_10","type":"FLOAT","widget":{"name":"block_10"},"link":null},{"localized_name":"block_11","name":"block_11","type":"FLOAT","widget":{"name":"block_11"},"link":null},{"localized_name":"block_12","name":"block_12","type":"FLOAT","widget":{"name":"block_12"},"link":null},{"localized_name":"block_13","name":"block_13","type":"FLOAT","widget":{"name":"block_13"},"link":null},{"localized_name":"block_14","name":"block_14","type":"FLOAT","widget":{"name":"block_14"},"link":null},{"localized_name":"block_15","name":"block_15","type":"FLOAT","widget":{"name":"block_15"},"link":null},{"localized_name":"block_16","name":"block_16","type":"FLOAT","widget":{"name":"block_16"},"link":null},{"localized_name":"block_17","name":"block_17","type":"FLOAT","widget":{"name":"block_17"},"link":null},{"localized_name":"block_18","name":"block_18","type":"FLOAT","widget":{"name":"block_18"},"link":null},{"localized_name":"block_19","name":"block_19","type":"FLOAT","widget":{"name":"block_19"},"link":null},{"localized_name":"block_20","name":"block_20","type":"FLOAT","widget":{"name":"block_20"},"link":null},{"localized_name":"block_21","name":"block_21","type":"FLOAT","widget":{"name":"block_21"},"link":null},{"localized_name":"block_22","name":"block_22","type":"FLOAT","widget":{"name":"block_22"},"link":null},{"localized_name":"block_23","name":"block_23","type":"FLOAT","widget":{"name":"block_23"},"link":null},{"localized_name":"block_24","name":"block_24","type":"FLOAT","widget":{"name":"block_24"},"link":null},{"localized_name":"block_25","name":"block_25","type":"FLOAT","widget":{"name":"block_25"},"link":null},{"localized_name":"block_26","name":"block_26","type":"FLOAT","widget":{"name":"block_26"},"link":null},{"localized_name":"block_27","name":"block_27","type":"FLOAT","widget":{"name":"block_27"},"link":null},{"localized_name":"block_28","name":"block_28","type":"FLOAT","widget":{"name":"block_28"},"link":null},{"localized_name":"block_29","name":"block_29","type":"FLOAT","widget":{"name":"block_29"},"link":null}],"outputs":[{"localized_name":"LORA_STACK","name":"LORA_STACK","type":"LORA_STACK","links":[186]},{"localized_name":"ANALYSIS_REPORT","name":"ANALYSIS_REPORT","type":"STRING","links":[187]}],"properties":{"Node name for S&R":"ZFuseVisualLayerTuner","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.5.2"}},"widgets_values":["UwU_Clarity_Zimg_000018750.safetensors",1,0,0,0,0,0,0,0,0,0,0,1.5,1.5,1.5,1.4,1.4,1.4,1.15,1.3,1.35,0,0,0,0,0,0,0,0,0,0,0],"color":"#323","bgcolor":"#535"},{"id":84,"type":"Note","pos":[560,2460],"size":[410,360],"flags":{},"order":12,"mode":0,"inputs":[],"outputs":[],"title":"Technical Requirements: Recommended Specs","properties":{"ue_properties":{"widget_ue_connectable":{},"version":"7.5.2","input_ue_unconnectable":{}}},"widgets_values":["Without going into too much technical babble, this is a heavy node! It eats a lot of ram due to how we re-make loras and then subsequently merge them.\n\nWe convert between float16 and float32 for calculations and store block layers into CPU RAM. So the more loras we use, the more RAM will be required. The GPU runs calculations for speed and the CPU holds information. These blocks will stay \"live\" for subsequent generations. To save your RAM its reccomended to export and save the final lora when you're happy with your generations.\n\nRecommended Specs: (<3 or more Loras)\n-64GB+ RAM\n-12GB VRAM\n\nMinimum Specs: (>2 Loras)\n-32GB RAM\n-8GB VRAM"],"color":"#322","bgcolor":"#533"},{"id":30,"type":"Note","pos":[1850,2460],"size":[410,360],"flags":{},"order":13,"mode":0,"inputs":[],"outputs":[],"title":"How to merge by block for Z-Image","properties":{"ue_properties":{"widget_ue_connectable":{},"version":"7.5.2","input_ue_unconnectable":{}}},"widgets_values":["Control Tier\nBlock Range\nZ-FUSE Operational Strategy\n\nConcept / Pose\nBlocks 0–6\nAssign 100% weight to \"Concept\" LoRAs; 0% to \"Detail\" LoRAs to keep composition clean.\n\nAnatomy Bypass\nBlocks 10–18\nAmplify specialized anatomy LoRAs (up to 1.5x) to override base model sterilization.\n\nTexture / Clarity\nBlocks 22–30\nTarget these layers for skin texture, wetness, or freckle LoRAs without warping the underlying pose.\n\nThese are a general rule of thumb and might not apply to every Lora model. As weights congregate in certain layers based on different training methods. Looking at layer weights is the best way to determine hot layers for each lora."],"color":"#432","bgcolor":"#653"},{"id":34,"type":"Note","pos":[4190,3820],"size":[230,110],"flags":{},"order":14,"mode":0,"inputs":[],"outputs":[],"title":"Save Lora Model","properties":{"ue_properties":{"widget_ue_connectable":{},"version":"7.5.2","input_ue_unconnectable":{}}},"widgets_values":["Saves the resulting Lora with preservation. New lora should generate the same images as the merged blocks. This keeps generations the same with no loss or change to the quality of the image. RANK 0 = Auto (Lossless)"],"color":"#432","bgcolor":"#653"},{"id":33,"type":"easy showAnything","pos":[4690,3660],"size":[250,270],"flags":{},"order":32,"mode":0,"inputs":[{"localized_name":"anything","name":"anything","shape":7,"type":"*","link":190}],"outputs":[{"localized_name":"output","name":"output","type":"*","links":null}],"title":"Save Status","properties":{"Node name for S&R":"easy showAnything","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.5.2"}},"widgets_values":["<comfy.model_patcher.ModelPatcher object at 0x00000257A548D700>"]},{"id":102,"type":"ZFuseBake","pos":[4430,3660],"size":[248.847265625,154],"flags":{},"order":26,"mode":4,"inputs":[{"localized_name":"model","name":"model","type":"MODEL","link":188},{"localized_name":"save_name","name":"save_name","type":"STRING","widget":{"name":"save_name"},"link":null},{"localized_name":"rank","name":"rank","type":"INT","widget":{"name":"rank"},"link":null},{"localized_name":"precision","name":"precision","type":"COMBO","widget":{"name":"precision"},"link":null},{"localized_name":"custom_path","name":"custom_path","type":"STRING","widget":{"name":"custom_path"},"link":189},{"localized_name":"vram_protection","name":"vram_protection","type":"BOOLEAN","widget":{"name":"vram_protection"},"link":null}],"outputs":[{"localized_name":"S","name":"S","type":"STRING","links":[190]}],"properties":{"Node name for S&R":"ZFuseBake","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.5.2"}},"widgets_values":["z_fuse_export",16,"float16","",true],"color":"#2a363b","bgcolor":"#3f5159"},{"id":44,"type":"Text Multiline","pos":[3120,3390],"size":[610,540],"flags":{},"order":15,"mode":0,"inputs":[{"localized_name":"text","name":"text","type":"STRING","widget":{"name":"text"},"link":null}],"outputs":[{"localized_name":"STRING","name":"STRING","type":"STRING","links":[65]}],"title":"Prompt","properties":{"Node name for S&R":"Text Multiline","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.5.2"}},"widgets_values":["(NSFW:10), (sexy:10), (explicit content:10), (uncensored:10), Fnsfw.\n\n\n\n#Subject - Margarita Nikolaevna, a stunning woman with a pronounced hourglass figure and large breasts, exudes an aura of natural beauty and refined taste. Her short, neat haircut frames her face perfectly, originally styled in a salon perm but now sporting natural curls thanks to the magic Azazello cream. Vivid eyes sparkle with inner fire and passion, constantly radiating an incomprehensible light that draws the viewer in. Her skin glows youthful and radiant under the studio lights.\n\n#Composition - Captured in a portrait photography setting, Margarita stands confidently against a plain backdrop. The framing emphasizes her graceful pose and elegant bearing as she looks directly at the camera with those mesmerizing eyes.\n\n#Background - A professional photo studio provides the neutral backdrop for this intimate portrait session. The clean lines and muted tones allow Margarita's natural beauty to take center stage without distraction.\n\n#Lighting - Studio lights cast soft shadows across Margarita's curves while highlighting every detail of her flawless skin and luxurious hair. The warm glow accentuates her vibrant eyes and full lips, inviting the viewer to lose themselves in her captivating gaze.\n\n\n\n\nBright light illuminates the scene from above, casting harsh, defined shadows.\n#realistic side lighting, realistic soft lighting, \n\n\n\n\n\n\n"],"color":"#232","bgcolor":"#353"},{"id":77,"type":"easy showAnything","pos":[3740,3790],"size":[300,130],"flags":{},"order":27,"mode":0,"inputs":[{"localized_name":"anything","name":"anything","shape":7,"type":"*","link":155}],"outputs":[{"localized_name":"output","name":"output","type":"*","links":[191]}],"title":"Extracted Embedded Trigger Words","properties":{"Node name for S&R":"easy showAnything","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.5.2"}},"widgets_values":["1_sexually_explicit, 1_0lympu5"],"color":"#223","bgcolor":"#335"},{"id":103,"type":"StringReplace","pos":[4050,3820],"size":[400,200],"flags":{"collapsed":true},"order":33,"mode":0,"inputs":[{"localized_name":"string","name":"string","type":"STRING","widget":{"name":"string"},"link":191},{"localized_name":"find","name":"find","type":"STRING","widget":{"name":"find"},"link":null},{"localized_name":"replace","name":"replace","type":"STRING","widget":{"name":"replace"},"link":null}],"outputs":[{"localized_name":"STRING","name":"STRING","type":"STRING","links":[192]}],"properties":{"ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{}},"Node name for S&R":"StringReplace"},"widgets_values":["","1_",""]},{"id":45,"type":"easy showAnything","pos":[3740,2970],"size":[410,330],"flags":{},"order":36,"mode":0,"inputs":[{"localized_name":"anything","name":"anything","shape":7,"type":"*","link":66}],"outputs":[{"localized_name":"output","name":"output","type":"*","links":[]}],"title":"Full Prompt + Extracted Keyword + Tags","properties":{"Node name for S&R":"easy showAnything","ue_properties":{"widget_ue_connectable":{},"input_ue_unconnectable":{},"version":"7.5.2"}},"widgets_values":["sexually_explicit, 0lympu5, uwu_clarity, masterpiece, best quality, very aesthetic,, (NSFW:10), (sexy:10), (explicit content:10), (uncensored:10), Fnsfw.\n\n\n\n\n\n\n\n\n\n\nBright light illuminates the scene from above, casting harsh, defined shadows."],"color":"#232","bgcolor":"#353"}],"links":[[5,5,0,3,2,"CONDITIONING"],[6,6,0,3,3,"LATENT"],[8,3,0,8,0,"LATENT"],[48,37,0,9,0,"IMAGE"],[49,8,0,37,1,"IMAGE"],[51,5,0,38,2,"CONDITIONING"],[52,6,0,38,3,"LATENT"],[53,2,0,38,0,"MODEL"],[54,38,0,39,0,"LATENT"],[55,39,0,37,0,"IMAGE"],[62,42,0,4,1,"STRING"],[64,43,0,42,1,"STRING"],[65,44,0,42,2,"STRING"],[66,42,0,45,0,"STRING"],[151,2,0,79,0,"MODEL"],[155,79,1,77,0,"STRING"],[156,79,2,75,0,"STRING"],[157,79,3,74,0,"STRING"],[158,79,4,76,0,"STRING"],[161,71,1,83,0,"STRING"],[162,71,0,79,1,"LORA_STACK"],[168,88,0,94,0,"MODEL"],[169,90,0,4,0,"CLIP"],[170,7,0,90,0,"CLIP"],[171,89,0,3,1,"CONDITIONING"],[172,89,0,5,0,"CONDITIONING"],[173,89,0,38,1,"CONDITIONING"],[174,4,0,89,0,"CONDITIONING"],[175,95,0,38,4,"INT"],[176,95,0,3,4,"INT"],[177,79,0,88,0,"MODEL"],[178,94,0,3,0,"MODEL"],[180,96,1,85,0,"STRING"],[181,96,0,97,0,"LORA_STACK"],[182,97,0,71,0,"LORA_STACK"],[183,97,1,82,0,"STRING"],[186,101,0,96,0,"LORA_STACK"],[187,101,1,99,0,"STRING"],[188,79,0,102,0,"MODEL"],[189,46,0,102,4,"STRING"],[190,102,0,33,0,"STRING"],[191,77,0,103,0,"STRING"],[192,103,0,42,0,"STRING"]],"groups":[{"id":1,"title":"Lora Stackers + Visual Data -- I recommend Only enabling these node first to see analysis reports.","bounding":[550,2890,2540,1050],"color":"#3f789e","font_size":24,"flags":{}},{"id":2,"title":"INFO + Technical","bounding":[550,2390,2540,490],"color":"#3f789e","font_size":24,"flags":{}},{"id":3,"title":"Models + Prompting","bounding":[3110,2900,1050,1040],"color":"#3f789e","font_size":24,"flags":{}},{"id":4,"title":"Saving + Sampling","bounding":[4170,2900,780,1040],"color":"#3f789e","font_size":24,"flags":{}},{"id":5,"title":"LOGS & REPORTS","bounding":[3960,2390,990,503.6],"color":"#3f789e","font_size":24,"flags":{}}],"config":{},"extra":{"workflowRendererVersion":"LG","ue_links":[],"links_added_by_ue":[],"ds":{"scale":1.0834705943389418,"offset":[-3031.8164846374707,-3002.340545777513]}},"version":0.4}